{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596710132517",
   "display_name": "Python 3.8.3 64-bit ('torchenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoCorrectusing Facebook Data\n",
    "\n",
    "## Creating vocabulary\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "BASE_DIR = \"inbox\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define text cleaner\n",
    "Perform following tasks:\n",
    "- Strip special escape characters\n",
    "- remove most common punctuations\n",
    "- Remove exra spaces\n",
    "- Remove links and emojis (done in next function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.strip().lower()\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    text = re.sub(r\"https?://.+\", r\"\", text)\n",
    "    text = re.sub(\"([.,!?():;])\", r' ', text)\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    return re.sub(r' +', r' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Processed 60 chats\nProcessed 120 chats\nProcessed 180 chats\nProcessed 240 chats\nProcessed 300 chats\nProcessed 360 chats\nProcessed 420 chats\nProcessed 480 chats\nProcessed 540 chats\nProcessed 600 chats\nProcessed 660 chats\n"
    }
   ],
   "source": [
    "chat_folders = os.listdir(BASE_DIR)\n",
    "\n",
    "all_sentences = []\n",
    "\n",
    "for i, chat in enumerate(chat_folders):\n",
    "    files = os.listdir(os.path.join(BASE_DIR, chat))\n",
    "    msg_files = list(filter(lambda x: x.startswith(\"message\"), files))\n",
    "    for msg_file in msg_files:\n",
    "        with open(os.path.join(BASE_DIR, chat, msg_file), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        if len(data['participants']) == 2:\n",
    "\n",
    "            msgs = data['messages']\n",
    "\n",
    "            for msg in msgs:\n",
    "                if msg.get('content', None) and msg['sender_name'] == 'Sahil Aggrawal':\n",
    "                    sentence = clean_text(msg['content']).split()\n",
    "                    if len(sentence) > 3:\n",
    "                        all_sentences.append(sentence)\n",
    "\n",
    "    if (i + 1) % 60 == 0 :\n",
    "        print(f\"Processed {i+1} chats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "53222"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "len(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "\n",
    "random.shuffle(all_sentences)\n",
    "\n",
    "train_size = int(len(all_sentences) * 0.8)\n",
    "\n",
    "train_data = all_sentences[:train_size]\n",
    "test_data = all_sentences[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Length of Train set : 42577\nLength of Test set : 10645\n"
    }
   ],
   "source": [
    "print(f\"Length of Train set : {len(train_data)}\\nLength of Test set : {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter()\n",
    "\n",
    "for sent in train_data:\n",
    "    word_count.update(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(word_count.most_common(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(data, vocab):\n",
    "    processed_sent = []\n",
    "    for i, sent in enumerate(data):\n",
    "        s = []\n",
    "        for j, token in enumerate(sent):\n",
    "            if token not in vocab.keys():\n",
    "                s.append(\"<UNK>\")\n",
    "            else:\n",
    "                s.append(token)\n",
    "\n",
    "        processed_sent.append(s)\n",
    "\n",
    "    return processed_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess(train_data, vocab)\n",
    "test = preprocess(test_data, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data_and_vocab.p\", \"wb\") as f:\n",
    "    pickle.dump([train, test, vocab], f)"
   ]
  }
 ]
}