{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596868975269",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_and_vocab.p\", \"rb\") as f:\n",
    "    train_data, test_data, vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "42577"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram(sentences, n_gram):\n",
    "\n",
    "    n_gram_count = {}\n",
    "\n",
    "    for sent in sentences:\n",
    "        s = [\"<s>\"] * (n_gram - 1) + sent + [\"<e>\"]\n",
    "\n",
    "        s = tuple(s)\n",
    "\n",
    "        m = len(s) if n_gram == 1 else len(s) - (n_gram - 1)\n",
    "\n",
    "        for i in range(m):\n",
    "            n_gram_count[s[i:i+n_gram]] = n_gram_count.get(s[i:i+n_gram], 0) + 1\n",
    "\n",
    "    return n_gram_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bigram_counts = get_n_gram(train_data, 2)\n",
    "unigram_count = get_n_gram(train_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_grams, n_gram_counts, n_plus1_gram_counts, vocab_size, k = 1.0):\n",
    "    prev = tuple(previous_n_grams)\n",
    "\n",
    "    denominator = n_gram_counts.get(prev, 0) + k * vocab_size\n",
    "\n",
    "    ngram = prev + (word,)\n",
    "\n",
    "    numerator = n_plus1_gram_counts.get(ngram, 0) + k\n",
    "\n",
    "    return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('drop', 'kar')\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.0011949810794662419"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "gram = list(bigram_counts.keys())[200]\n",
    "print(gram)\n",
    "previous_gram = gram[:-1]\n",
    "previous_gram\n",
    "\n",
    "estimate_probability(gram[1], previous_gram, unigram_count, bigram_counts, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_all_probabilities(given_words, n_gram_count, n_plus1_gram_count, vocabulary, k = 1.0):\n",
    "    n = len(list(n_plus1_gram_count.keys())[0])\n",
    "    \n",
    "    if isinstance(given_words, str):\n",
    "        previous_gram = tuple([given_words,])\n",
    "    else:\n",
    "        previous_gram = tuple(given_words)\n",
    "\n",
    "    vocabulary = list(vocab.keys()) + [\"<e>\", \"<UNK>\"]\n",
    "\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        x = estimate_probability(word, previous_gram, n_gram_count, n_plus1_gram_count, len(vocabulary))\n",
    "        probabilities[word] = x\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = estimate_all_probabilities(\"drop\", unigram_count, bigram_counts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_items = list(x.items())\n",
    "\n",
    "z = sorted(x_items, key=lambda s: s[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('kar', 0.0011945052757316346),\n ('karke', 0.0007963368504877563),\n ('ka', 0.0005972526378658173),\n ('karunga', 0.0005972526378658173),\n ('one', 0.0005972526378658173)]"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "z[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create count and probability matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "    vocab = list(vocabulary.keys()) + [\"<e>\", \"<UNK>\"]\n",
    "\n",
    "    n_grams = []\n",
    "\n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[:-1]\n",
    "        n_grams.append(n_gram)\n",
    "\n",
    "    n_grams = list(set(n_grams))\n",
    "\n",
    "    rows = {gram : i for i, gram in enumerate(n_grams)}\n",
    "\n",
    "    cols = {word : j for j, word in enumerate(vocab)}\n",
    "\n",
    "    n_rows = len(rows)\n",
    "    n_cols = len(cols)\n",
    "\n",
    "    count_matrix = np.zeros((n_rows, n_cols))\n",
    "\n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        ngram = n_plus1_gram[:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        if word not in vocab:\n",
    "            continue\n",
    "\n",
    "        i = rows.get(ngram)\n",
    "        j = cols.get(word)\n",
    "\n",
    "        count_matrix[i, j] = count\n",
    "\n",
    "    return pd.DataFrame(count_matrix, index = n_grams, columns = vocab)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trigram_counts = get_n_gram(train_data, 3)\n",
    "cmatrix = make_count_matrix(bigram_counts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             ha    to   me   ho  hai   he    b   ni  kar  aur  ...  fed  \\\n(jyada,)    0.0   1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n(topic,)    8.0   1.0  1.0  1.0  5.0  3.0  1.0  7.0  1.0  1.0  ...  0.0   \n(chalaya,)  0.0   0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n(air,)      0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n(thoda,)    0.0  12.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  3.0  ...  0.0   \n\n            stars  html5  hcl  woman  ankush  reg  domain   <e>  <UNK>  \n(jyada,)      0.0    0.0  0.0    0.0     0.0  0.0     0.0   0.0    1.0  \n(topic,)      0.0    0.0  0.0    0.0     0.0  0.0     0.0  23.0    5.0  \n(chalaya,)    0.0    0.0  0.0    0.0     0.0  0.0     0.0   0.0    0.0  \n(air,)        0.0    0.0  0.0    0.0     0.0  0.0     0.0   0.0    0.0  \n(thoda,)      0.0    0.0  0.0    0.0     0.0  0.0     0.0  10.0   13.0  \n\n[5 rows x 5002 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ha</th>\n      <th>to</th>\n      <th>me</th>\n      <th>ho</th>\n      <th>hai</th>\n      <th>he</th>\n      <th>b</th>\n      <th>ni</th>\n      <th>kar</th>\n      <th>aur</th>\n      <th>...</th>\n      <th>fed</th>\n      <th>stars</th>\n      <th>html5</th>\n      <th>hcl</th>\n      <th>woman</th>\n      <th>ankush</th>\n      <th>reg</th>\n      <th>domain</th>\n      <th>&lt;e&gt;</th>\n      <th>&lt;UNK&gt;</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>(jyada,)</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>(topic,)</th>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>23.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>(chalaya,)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(air,)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(thoda,)</th>\n      <td>0.0</td>\n      <td>12.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>10.0</td>\n      <td>13.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 5002 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "cmatrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix += 1\n",
    "prob_matrix = cmatrix.div(cmatrix.sum(axis = 1), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                  ha        to        me        ho       hai        he  \\\n(jyada,)    0.000200  0.000399  0.000200  0.000200  0.000200  0.000200   \n(topic,)    0.001744  0.000388  0.000388  0.000388  0.001163  0.000775   \n(chalaya,)  0.000200  0.000200  0.000200  0.000200  0.000400  0.000200   \n(air,)      0.000199  0.000199  0.000199  0.000199  0.000199  0.000199   \n(thoda,)    0.000193  0.002512  0.000193  0.000193  0.000193  0.000386   \n\n                   b        ni       kar       aur  ...       fed     stars  \\\n(jyada,)    0.000200  0.000200  0.000200  0.000200  ...  0.000200  0.000200   \n(topic,)    0.000388  0.001550  0.000388  0.000388  ...  0.000194  0.000194   \n(chalaya,)  0.000200  0.000200  0.000200  0.000200  ...  0.000200  0.000200   \n(air,)      0.000199  0.000199  0.000199  0.000199  ...  0.000199  0.000199   \n(thoda,)    0.000193  0.000193  0.000193  0.000773  ...  0.000193  0.000193   \n\n               html5       hcl     woman    ankush       reg    domain  \\\n(jyada,)    0.000200  0.000200  0.000200  0.000200  0.000200  0.000200   \n(topic,)    0.000194  0.000194  0.000194  0.000194  0.000194  0.000194   \n(chalaya,)  0.000200  0.000200  0.000200  0.000200  0.000200  0.000200   \n(air,)      0.000199  0.000199  0.000199  0.000199  0.000199  0.000199   \n(thoda,)    0.000193  0.000193  0.000193  0.000193  0.000193  0.000193   \n\n                 <e>     <UNK>  \n(jyada,)    0.000200  0.000399  \n(topic,)    0.004650  0.001163  \n(chalaya,)  0.000200  0.000200  \n(air,)      0.000199  0.000199  \n(thoda,)    0.002125  0.002705  \n\n[5 rows x 5002 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ha</th>\n      <th>to</th>\n      <th>me</th>\n      <th>ho</th>\n      <th>hai</th>\n      <th>he</th>\n      <th>b</th>\n      <th>ni</th>\n      <th>kar</th>\n      <th>aur</th>\n      <th>...</th>\n      <th>fed</th>\n      <th>stars</th>\n      <th>html5</th>\n      <th>hcl</th>\n      <th>woman</th>\n      <th>ankush</th>\n      <th>reg</th>\n      <th>domain</th>\n      <th>&lt;e&gt;</th>\n      <th>&lt;UNK&gt;</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>(jyada,)</th>\n      <td>0.000200</td>\n      <td>0.000399</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>...</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000399</td>\n    </tr>\n    <tr>\n      <th>(topic,)</th>\n      <td>0.001744</td>\n      <td>0.000388</td>\n      <td>0.000388</td>\n      <td>0.000388</td>\n      <td>0.001163</td>\n      <td>0.000775</td>\n      <td>0.000388</td>\n      <td>0.001550</td>\n      <td>0.000388</td>\n      <td>0.000388</td>\n      <td>...</td>\n      <td>0.000194</td>\n      <td>0.000194</td>\n      <td>0.000194</td>\n      <td>0.000194</td>\n      <td>0.000194</td>\n      <td>0.000194</td>\n      <td>0.000194</td>\n      <td>0.000194</td>\n      <td>0.004650</td>\n      <td>0.001163</td>\n    </tr>\n    <tr>\n      <th>(chalaya,)</th>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000400</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>...</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <th>(air,)</th>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>...</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n      <td>0.000199</td>\n    </tr>\n    <tr>\n      <th>(thoda,)</th>\n      <td>0.000193</td>\n      <td>0.002512</td>\n      <td>0.000193</td>\n      <td>0.000193</td>\n      <td>0.000193</td>\n      <td>0.000386</td>\n      <td>0.000193</td>\n      <td>0.000193</td>\n      <td>0.000193</td>\n      <td>0.000773</td>\n      <td>...</td>\n      <td>0.000193</td>\n      <td>0.000193</td>\n      <td>0.000193</td>\n      <td>0.000193</td>\n      <td>0.000193</td>\n      <td>0.000193</td>\n      <td>0.000193</td>\n      <td>0.000193</td>\n      <td>0.002125</td>\n      <td>0.002705</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 5002 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "prob_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocab_size, k = 1.0):\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "\n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "\n",
    "    sentence = tuple(sentence)\n",
    "\n",
    "    N = len(sentence)\n",
    "\n",
    "    logsum = 0\n",
    "\n",
    "    for t in range(n, N):\n",
    "        n_gram = sentence[t - n : t]\n",
    "        word = sentence[t]\n",
    "\n",
    "        prob = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocab_size, k = k)\n",
    "\n",
    "        logsum += np.log(prob)\n",
    "    \n",
    "    logsum = (1 / N) * logsum\n",
    "    perplexity = 2 ** (-logsum)\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_over_sentences(sentences, vocab, n_gram_counts, n_plus1_gram_counts, k = 1):\n",
    "    vocab = list(vocab.keys()) + [\"<e>\", \"<UNK>\"]\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    p = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        p += compute_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocab_size, k = k)\n",
    "\n",
    "    return p / len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perplexity = compute_perplexity_over_sentences(train_data, vocab, unigram_count, bigram_counts)\n",
    "test_perplexity = compute_perplexity_over_sentences(test_data, vocab, unigram_count, bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Perplexity of Training set : 36.53088\nPerplexity of Testing set : 40.98047\n"
    }
   ],
   "source": [
    "print(f\"Perplexity of Training set : {train_perplexity:.5f}\\nPerplexity of Testing set : {test_perplexity:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# suggest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_word(previous_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k = 1.0, start_with = None):\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "\n",
    "    previous_n_token = previous_words[-n:]\n",
    "\n",
    "    probs = estimate_all_probabilities(previous_n_token, n_gram_counts, n_plus1_gram_counts, vocabulary, k = k)\n",
    "\n",
    "    max_prob = 0\n",
    "    suggestion = None\n",
    "\n",
    "    for word, prob in probs.items():\n",
    "        if start_with:\n",
    "            if not word.startswith(start_with):\n",
    "                continue\n",
    "\n",
    "        if prob > max_prob:\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "\n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.strip().lower()\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    text = re.sub(r\"https?://.+\", r\"\", text)\n",
    "    text = re.sub(\"([.,!?():;])\", r' ', text)\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    return re.sub(r' +', r' ', text)\n",
    "\n",
    "def preprocess(data, vocab):\n",
    "    processed_sent = []\n",
    "    for j, token in enumerate(data):\n",
    "        if token not in vocab.keys():\n",
    "            processed_sent.append(\"<UNK>\")\n",
    "        else:\n",
    "            processed_sent.append(token)\n",
    "\n",
    "    return processed_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"Kaha jaa rahe\"\n",
    "cleaned = clean_text(x).split()\n",
    "text = preprocess(cleaned, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "('hai', 0.00633619083115915)"
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "suggest_word(text, unigram_count, bigram_counts, vocab, start_with=\"hai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Computing n grm counts for n =  1 ...\nComputing n grm counts for n =  2 ...\nComputing n grm counts for n =  3 ...\nComputing n grm counts for n =  4 ...\nComputing n grm counts for n =  5 ...\n"
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n grm counts for n = \", n, \"...\")\n",
    "    n_model_counts = get_n_gram(train_data, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_suggestions(previous_words, n_gram_counts_list, vocabulary, k = 1.0, start_with=None):\n",
    "    model_count = len(n_gram_counts_list)\n",
    "\n",
    "    suggetions = []\n",
    "    for i in range(model_count - 1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i + 1]\n",
    "\n",
    "        s = suggest_word(previous_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k = k, start_with=start_with)\n",
    "\n",
    "        suggetions.append(s)\n",
    "    \n",
    "    return suggetions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('ho', 0.026649273201639956),\n ('ha', 0.0011961722488038277),\n ('the', 0.0003997601439136518),\n ('ha', 0.00019992003198720512)]"
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "get_multiple_suggestions(text, n_gram_counts_list, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}